{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7CluGh9mXbMu",
        "outputId": "417de651-4eb2-4d19-9e4b-df041e1f398a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sztmJN0qIsqT",
        "outputId": "e46e30e9-f673-4d94-ca67-3dd4c51e3714"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "==================================================\n",
            "✓ Google Drive mounted successfully\n",
            "✓ Backup directory: /content/drive/MyDrive/amazon_reviews_backup\n",
            "==================================================\n",
            "\n",
            "\n",
            "==================================================\n",
            "AMAZON REVIEWS PROCESSOR\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "STORAGE USAGE\n",
            "==================================================\n",
            "Processed parquet files: 0.0GB\n",
            "Total space freed so far: 0.0GB\n",
            "\n",
            "==================================================\n",
            "STARTING PROCESSING\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "CHECKING CURRENT PROGRESS\n",
            "==================================================\n",
            "\n",
            "Found in Drive backup: 34 categories\n",
            "Found in local processed: 0 categories\n",
            "Fetching available categories...\n",
            "\n",
            "WARNING: Found files in Drive not marked as completed:\n",
            "- Sports_and_Outdoors\n",
            "- Software\n",
            "- CDs_and_Vinyl\n",
            "- Amazon_Fashion\n",
            "- Automotive\n",
            "- Health_and_Household\n",
            "- Video_Games\n",
            "- Beauty_and_Personal_Care\n",
            "- Subscription_Boxes\n",
            "- Baby_Products\n",
            "- Tools_and_Home_Improvement\n",
            "- Magazine_Subscriptions\n",
            "- Handmade_Products\n",
            "- Arts_Crafts_and_Sewing\n",
            "- Books\n",
            "- Cell_Phones_and_Accessories\n",
            "- Office_Products\n",
            "- Kindle_Store\n",
            "- Movies_and_TV\n",
            "- Clothing_Shoes_and_Jewelry\n",
            "- Digital_Music\n",
            "- Musical_Instruments\n",
            "- Electronics\n",
            "- All_Beauty\n",
            "- Patio_Lawn_and_Garden\n",
            "- Toys_and_Games\n",
            "- Pet_Supplies\n",
            "- Gift_Cards\n",
            "- Appliances\n",
            "- Industrial_and_Scientific\n",
            "- Home_and_Kitchen\n",
            "- Grocery_and_Gourmet_Food\n",
            "- Health_and_Personal_Care\n",
            "- Unknown\n",
            "\n",
            "SUMMARY:\n",
            "Total categories: 34\n",
            "Completed: 34\n",
            "Remaining: 0\n",
            "\n",
            "==================================================\n",
            "\n",
            "PROCESS COMPLETE\n",
            "==================================================\n",
            "Total time: 0.0 minutes\n",
            "Total reviews processed: 0\n",
            "Log file: download_log_20241130_130048.txt\n",
            "\n",
            "==================================================\n",
            "VERIFYING FILES\n",
            "==================================================\n",
            "Checking raw files in: /content/drive/MyDrive/amazon_reviews_backup\n",
            "✓ Gift_Cards.parquet: 152,410 reviews\n",
            "✓ Baby_Products.parquet: 6,028,884 reviews\n",
            "✓ Cell_Phones_and_Accessories.parquet: 20,812,945 reviews\n",
            "✓ Handmade_Products.parquet: 664,162 reviews\n",
            "✓ Musical_Instruments.parquet: 3,017,439 reviews\n",
            "✓ Industrial_and_Scientific.parquet: 5,183,005 reviews\n",
            "✓ Electronics.parquet: 43,886,943 reviews\n",
            "✓ Arts_Crafts_and_Sewing.parquet: 8,966,758 reviews\n",
            "✓ Toys_and_Games.parquet: 16,260,406 reviews\n",
            "✓ All_Beauty.parquet: 701,528 reviews\n",
            "✓ Health_and_Household.parquet: 25,631,345 reviews\n",
            "✓ Office_Products.parquet: 12,845,712 reviews\n",
            "✓ Digital_Music.parquet: 130,434 reviews\n",
            "✓ Grocery_and_Gourmet_Food.parquet: 14,318,520 reviews\n",
            "✓ Tools_and_Home_Improvement.parquet: 26,982,256 reviews\n",
            "✓ Appliances.parquet: 2,128,605 reviews\n",
            "✓ Magazine_Subscriptions.parquet: 71,497 reviews\n",
            "✓ Subscription_Boxes.parquet: 16,216 reviews\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-c1197923ce6d>\u001b[0m in \u001b[0;36m<cell line: 470>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0;31m# Always verify files and show storage status at the end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m         \u001b[0mverify_parquet_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m         \u001b[0mcheck_storage_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nTotal space freed: {TOTAL_SPACE_FREED:.1f}GB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-c1197923ce6d>\u001b[0m in \u001b[0;36mverify_parquet_files\u001b[0;34m()\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{DRIVE_PATH}/{parquet_file}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"✓ {parquet_file}: {len(df):,} reviews\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[0mcheck_dtype_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype_backend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m     return impl.read(\n\u001b[0m\u001b[1;32m    668\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m         )\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             pa_table = self.api.parquet.read_table(\n\u001b[0m\u001b[1;32m    275\u001b[0m                 \u001b[0mpath_or_handle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/parquet/core.py\u001b[0m in \u001b[0;36mread_table\u001b[0;34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)\u001b[0m\n\u001b[1;32m   1841\u001b[0m         )\n\u001b[1;32m   1842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1843\u001b[0;31m     return dataset.read(columns=columns, use_threads=use_threads,\n\u001b[0m\u001b[1;32m   1844\u001b[0m                         use_pandas_metadata=use_pandas_metadata)\n\u001b[1;32m   1845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/parquet/core.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, columns, use_threads, use_pandas_metadata)\u001b[0m\n\u001b[1;32m   1483\u001b[0m                 )\n\u001b[1;32m   1484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1485\u001b[0;31m         table = self._dataset.to_table(\n\u001b[0m\u001b[1;32m   1486\u001b[0m             \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filter_expression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m             \u001b[0muse_threads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_threads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset, get_dataset_config_names, load_from_disk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "import sys\n",
        "import gc\n",
        "\n",
        "# Mount Google Drive\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    DRIVE_PATH = \"/content/drive/MyDrive/amazon_reviews_backup\"\n",
        "    os.makedirs(DRIVE_PATH, exist_ok=True)\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"✓ Google Drive mounted successfully\")\n",
        "    print(\"✓ Backup directory:\", DRIVE_PATH)\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "except Exception as e:\n",
        "    print(\"✗ Error mounting Google Drive:\", str(e))\n",
        "    sys.exit(1)\n",
        "\n",
        "# Track total space freed\n",
        "TOTAL_SPACE_FREED = 0\n",
        "\n",
        "def load_progress():\n",
        "    \"\"\"Load progress from Drive\"\"\"\n",
        "    progress_file = f'{DRIVE_PATH}/download_progress.json'\n",
        "    if os.path.exists(progress_file):\n",
        "        with open(progress_file, 'r') as f:\n",
        "            return json.load(f)\n",
        "    return {'completed': [], 'total_reviews': 0}\n",
        "\n",
        "def save_progress(completed_categories, total_reviews):\n",
        "    \"\"\"Save progress to Drive\"\"\"\n",
        "    progress_file = f'{DRIVE_PATH}/download_progress.json'\n",
        "    with open(progress_file, 'w') as f:\n",
        "        json.dump({\n",
        "            'completed': completed_categories,\n",
        "            'total_reviews': total_reviews,\n",
        "            'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "        }, f)\n",
        "\n",
        "def clear_memory():\n",
        "    \"\"\"Clear memory and run garbage collection\"\"\"\n",
        "    gc.collect()\n",
        "    if 'torch' in sys.modules:\n",
        "        import torch\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "def print_status(message, is_header=False):\n",
        "    \"\"\"Print formatted status messages\"\"\"\n",
        "    if is_header:\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(message)\n",
        "        print(\"=\"*50)\n",
        "    else:\n",
        "        print(message)\n",
        "\n",
        "def check_storage_usage():\n",
        "    \"\"\"Check current storage usage\"\"\"\n",
        "    print_status(\"STORAGE USAGE\", is_header=True)\n",
        "\n",
        "    # Check original dataset files\n",
        "    dataset_size = 0\n",
        "    if os.path.exists('amazon_reviews'):\n",
        "        dataset_size = sum(\n",
        "            sum(os.path.getsize(os.path.join(dirpath, filename))\n",
        "                for filename in filenames)\n",
        "            for dirpath, dirnames, filenames in os.walk('amazon_reviews')\n",
        "        ) / (1024**3)  # Convert to GB\n",
        "        print(f\"Original datasets: {dataset_size:.1f}GB\")\n",
        "\n",
        "    # Check processed files\n",
        "    parquet_size = 0\n",
        "    if os.path.exists('amazon_reviews_processed'):\n",
        "        parquet_size = sum(\n",
        "            os.path.getsize(os.path.join('amazon_reviews_processed', f))\n",
        "            for f in os.listdir('amazon_reviews_processed')\n",
        "        ) / (1024**3)  # Convert to GB\n",
        "        print(f\"Processed parquet files: {parquet_size:.1f}GB\")\n",
        "\n",
        "    print(f\"Total space freed so far: {TOTAL_SPACE_FREED:.1f}GB\")\n",
        "    return dataset_size, parquet_size\n",
        "\n",
        "def verify_parquet_integrity(file_path):\n",
        "    \"\"\"Verify parquet file is complete and readable\"\"\"\n",
        "    try:\n",
        "        with pd.read_parquet(file_path, columns=['rating']) as df:\n",
        "            return len(df) > 0\n",
        "    except Exception as e:\n",
        "        print(f\"Integrity check failed: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def cleanup_category(category):\n",
        "    \"\"\"Clean up local files for a category after successful backup\"\"\"\n",
        "    global TOTAL_SPACE_FREED\n",
        "    cleanup_info = {\n",
        "        'dataset_deleted': False,\n",
        "        'dataset_size': 0,\n",
        "        'parquet_deleted': False,\n",
        "        'parquet_size': 0\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        print(f\"\\nDEBUG - Starting cleanup for {category}\")\n",
        "\n",
        "        # Remove original dataset if it exists\n",
        "        dataset_path = f'amazon_reviews/{category}.dataset'\n",
        "        if os.path.exists(dataset_path):\n",
        "            print(f\"Found dataset at: {dataset_path}\")\n",
        "            if os.path.isdir(dataset_path):\n",
        "                cleanup_info['dataset_size'] = sum(\n",
        "                    os.path.getsize(os.path.join(dirpath, filename))\n",
        "                    for dirpath, dirnames, filenames in os.walk(dataset_path)\n",
        "                ) / (1024**3)\n",
        "                print(f\"Dataset size before deletion: {cleanup_info['dataset_size']:.1f}GB\")\n",
        "                shutil.rmtree(dataset_path)\n",
        "                cleanup_info['dataset_deleted'] = True\n",
        "                TOTAL_SPACE_FREED += cleanup_info['dataset_size']\n",
        "            else:\n",
        "                cleanup_info['dataset_size'] = os.path.getsize(dataset_path) / (1024**3)\n",
        "                os.remove(dataset_path)\n",
        "                cleanup_info['dataset_deleted'] = True\n",
        "                TOTAL_SPACE_FREED += cleanup_info['dataset_size']\n",
        "            print(f\"✓ Removed original dataset for {category}\")\n",
        "\n",
        "        # Remove local parquet if it exists\n",
        "        parquet_path = f'amazon_reviews_processed/{category}.parquet'\n",
        "        if os.path.exists(parquet_path):\n",
        "            print(f\"Found parquet at: {parquet_path}\")\n",
        "            cleanup_info['parquet_size'] = os.path.getsize(parquet_path) / (1024**3)\n",
        "            print(f\"Parquet size before deletion: {cleanup_info['parquet_size']:.1f}GB\")\n",
        "            os.remove(parquet_path)\n",
        "            cleanup_info['parquet_deleted'] = True\n",
        "            TOTAL_SPACE_FREED += cleanup_info['parquet_size']\n",
        "            print(f\"✓ Removed local parquet for {category}\")\n",
        "\n",
        "        total_freed = cleanup_info['dataset_size'] + cleanup_info['parquet_size']\n",
        "        print(f\"DEBUG - Cleanup complete. Freed {total_freed:.1f}GB\")\n",
        "        print(f\"Total space freed so far: {TOTAL_SPACE_FREED:.1f}GB\")\n",
        "        check_storage_usage()\n",
        "\n",
        "        return cleanup_info\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error during cleanup for {category}: {str(e)}\")\n",
        "        return cleanup_info\n",
        "\n",
        "def backup_to_drive(category, is_success=True):\n",
        "    \"\"\"Backup parquet file to Google Drive with integrity check\"\"\"\n",
        "    try:\n",
        "        if is_success:\n",
        "            source = f'amazon_reviews_processed/{category}.parquet'\n",
        "            temp_dest = f'{DRIVE_PATH}/temp_{category}.parquet'\n",
        "            final_dest = f'{DRIVE_PATH}/{category}.parquet'\n",
        "\n",
        "            # Copy to temporary location first\n",
        "            shutil.copy2(source, temp_dest)\n",
        "\n",
        "            # Verify integrity\n",
        "            if verify_parquet_integrity(temp_dest):\n",
        "                if os.path.exists(final_dest):\n",
        "                    os.remove(final_dest)\n",
        "                os.rename(temp_dest, final_dest)\n",
        "                print(f\"✓ Backed up {category} to Drive\")\n",
        "            else:\n",
        "                if os.path.exists(temp_dest):\n",
        "                    os.remove(temp_dest)\n",
        "                raise Exception(\"Backup verification failed\")\n",
        "\n",
        "        # Always backup the log file\n",
        "        log_files = [f for f in os.listdir('.') if f.startswith('download_log_')]\n",
        "        if log_files:\n",
        "            latest_log = max(log_files)  # Get most recent log\n",
        "            shutil.copy2(latest_log, f'{DRIVE_PATH}/{latest_log}')\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Error backing up to Drive: {str(e)}\")\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def check_disk_space():\n",
        "    \"\"\"Check available disk space\"\"\"\n",
        "    total, used, free = shutil.disk_usage(\"/\")\n",
        "    free_gb = free // (2**30)\n",
        "    return free_gb\n",
        "\n",
        "def get_review_categories():\n",
        "    \"\"\"Get list of all available review categories\"\"\"\n",
        "    print(\"Fetching available categories...\")\n",
        "    configs = get_dataset_config_names(\"McAuley-Lab/Amazon-Reviews-2023\")\n",
        "    review_categories = [\n",
        "        config.replace('raw_review_', '')\n",
        "        for config in configs\n",
        "        if config.startswith('raw_review_')\n",
        "    ]\n",
        "    return review_categories\n",
        "\n",
        "def verify_parquet_files():\n",
        "    \"\"\"Verify all raw parquet files are complete and readable\"\"\"\n",
        "    print_status(\"VERIFYING FILES\", is_header=True)\n",
        "\n",
        "    # Get all parquet files in the backup directory\n",
        "    parquet_files = [f for f in os.listdir(DRIVE_PATH) if f.endswith('.parquet')]\n",
        "\n",
        "    print(f\"Checking raw files in: {DRIVE_PATH}\")\n",
        "    for parquet_file in parquet_files:\n",
        "        file_path = f\"{DRIVE_PATH}/{parquet_file}\"\n",
        "        try:\n",
        "            df = pd.read_parquet(file_path)\n",
        "            print(f\"✓ {parquet_file}: {len(df):,} reviews\")\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Error reading {parquet_file}: {str(e)}\")\n",
        "\n",
        "    return True\n",
        "\n",
        "def check_progress():\n",
        "    \"\"\"Check current progress across all locations with improved reporting\"\"\"\n",
        "    print_status(\"CHECKING CURRENT PROGRESS\", is_header=True)\n",
        "\n",
        "    # Load saved progress\n",
        "    progress = load_progress()\n",
        "    completed_categories = set(progress['completed'])\n",
        "    if completed_categories:\n",
        "        print(f\"Found saved progress - {len(completed_categories)} categories completed\")\n",
        "        print(f\"Total reviews processed: {progress['total_reviews']:,}\")\n",
        "        print(f\"Last updated: {progress.get('last_updated', 'Unknown')}\")\n",
        "\n",
        "    # Check Drive backup\n",
        "    drive_files = set()\n",
        "    if os.path.exists(DRIVE_PATH):\n",
        "        drive_files = {\n",
        "            f.replace('.parquet', '')\n",
        "            for f in os.listdir(DRIVE_PATH)\n",
        "            if f.endswith('.parquet') and not f.startswith('temp_')\n",
        "        }\n",
        "        print(f\"\\nFound in Drive backup: {len(drive_files)} categories\")\n",
        "\n",
        "    # Check local processed files\n",
        "    local_files = set()\n",
        "    if os.path.exists('amazon_reviews_processed'):\n",
        "        local_files = {\n",
        "            f.replace('.parquet', '')\n",
        "            for f in os.listdir('amazon_reviews_processed')\n",
        "            if f.endswith('.parquet')\n",
        "        }\n",
        "        print(f\"Found in local processed: {len(local_files)} categories\")\n",
        "\n",
        "    # Check original datasets\n",
        "    dataset_files = set()\n",
        "    if os.path.exists('amazon_reviews'):\n",
        "        dataset_files = {\n",
        "            f.replace('.dataset', '')\n",
        "            for f in os.listdir('amazon_reviews')\n",
        "            if f.endswith('.dataset')\n",
        "        }\n",
        "        print(f\"Found original datasets: {len(dataset_files)} categories\")\n",
        "\n",
        "    # Get total categories\n",
        "    all_categories = set(get_review_categories())\n",
        "    remaining = all_categories - drive_files\n",
        "\n",
        "    # Identify inconsistencies\n",
        "    inconsistencies = drive_files - completed_categories\n",
        "    if inconsistencies:\n",
        "        print(\"\\nWARNING: Found files in Drive not marked as completed:\")\n",
        "        for cat in inconsistencies:\n",
        "            print(f\"- {cat}\")\n",
        "\n",
        "    print(\"\\nSUMMARY:\")\n",
        "    print(f\"Total categories: {len(all_categories)}\")\n",
        "    print(f\"Completed: {len(all_categories - remaining)}\")\n",
        "    print(f\"Remaining: {len(remaining)}\")\n",
        "\n",
        "    return drive_files, local_files, dataset_files, remaining\n",
        "\n",
        "def process_category(category, needed_columns, is_new_download=False):\n",
        "    \"\"\"Process a single category with improved error handling and retries\"\"\"\n",
        "    max_retries = 3\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            # Check disk space before processing\n",
        "            free_gb = check_disk_space()\n",
        "            if free_gb < 20:  # Set minimum required space to 20GB\n",
        "                print_status(\"WARNING: LOW DISK SPACE\", is_header=True)\n",
        "                print(f\"Only {free_gb}GB remaining\")\n",
        "                print(\"Waiting for user input...\")\n",
        "                response = input(\"Continue anyway? (yes/no): \")\n",
        "                if response.lower() != 'yes':\n",
        "                    print(\"Stopping process to prevent disk space issues\")\n",
        "                    return 0\n",
        "\n",
        "            print(f\"\\nDEBUG - Starting processing for {category} (Attempt {attempt + 1}/{max_retries})\")\n",
        "\n",
        "            if is_new_download:\n",
        "                print(\"Downloading new dataset...\")\n",
        "                config_name = f\"raw_review_{category}\"\n",
        "                dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\",\n",
        "                                     config_name,\n",
        "                                     trust_remote_code=True)\n",
        "                ds = dataset['full']\n",
        "            else:\n",
        "                print(\"Loading existing dataset...\")\n",
        "                ds = load_from_disk(f'amazon_reviews/{category}.dataset')['full']\n",
        "\n",
        "            print(\"Converting to pandas DataFrame...\")\n",
        "            df = pd.DataFrame({\n",
        "                col: ds[col]\n",
        "                for col in needed_columns\n",
        "                if col in ds.column_names\n",
        "            })\n",
        "\n",
        "            df['category'] = category\n",
        "\n",
        "            print(\"Saving as parquet...\")\n",
        "            os.makedirs('amazon_reviews_processed', exist_ok=True)\n",
        "            output_file = f'amazon_reviews_processed/{category}.parquet'\n",
        "            df.to_parquet(output_file)\n",
        "\n",
        "            num_reviews = len(df)\n",
        "            print(f\"Processed {num_reviews:,} reviews\")\n",
        "\n",
        "            # Clear memory\n",
        "            del ds\n",
        "            del df\n",
        "            clear_memory()\n",
        "\n",
        "            print(\"Backing up to Drive...\")\n",
        "            if backup_to_drive(category):\n",
        "                print(\"\\nVerifying backup and cleaning up...\")\n",
        "                cleanup_result = cleanup_category(category)\n",
        "                if cleanup_result['dataset_deleted'] or cleanup_result['parquet_deleted']:\n",
        "                    freed = cleanup_result['dataset_size'] + cleanup_result['parquet_size']\n",
        "                    print(f\"✓ Successfully cleaned up {freed:.1f}GB\")\n",
        "                else:\n",
        "                    print(\"! No files were deleted\")\n",
        "                return num_reviews\n",
        "            else:\n",
        "                if attempt < max_retries - 1:\n",
        "                    print(\"Backup failed, retrying...\")\n",
        "                    continue\n",
        "                else:\n",
        "                    print(\"All backup attempts failed\")\n",
        "                    return 0\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error on attempt {attempt + 1}: {str(e)}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                print(f\"Retrying in 5 seconds...\")\n",
        "                time.sleep(5)\n",
        "            else:\n",
        "                print(f\"All attempts failed for {category}\")\n",
        "                backup_to_drive(category, is_success=False)\n",
        "                return 0\n",
        "\n",
        "def transition_and_continue():\n",
        "    \"\"\"Main function with improved error handling and progress tracking\"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Load previous progress\n",
        "    progress = load_progress()\n",
        "    total_reviews = progress['total_reviews']\n",
        "    completed = set(progress['completed'])\n",
        "\n",
        "    print_status(\"STARTING PROCESSING\", is_header=True)\n",
        "\n",
        "    # Check current progress\n",
        "    drive_files, local_files, dataset_files, remaining = check_progress()\n",
        "\n",
        "    needed_columns = {\n",
        "        'text', 'rating', 'asin', 'title', 'helpful_vote'\n",
        "    }\n",
        "\n",
        "    os.makedirs('amazon_reviews_processed', exist_ok=True)\n",
        "    log_filename = f\"download_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
        "\n",
        "    try:\n",
        "        # First, convert existing .dataset files\n",
        "        if dataset_files:\n",
        "            print_status(f\"\\nProcessing {len(dataset_files)} existing downloads...\", is_header=True)\n",
        "\n",
        "            for category in dataset_files:\n",
        "                if category in completed:\n",
        "                    print(f\"Skipping {category} - already completed\")\n",
        "                    continue\n",
        "\n",
        "                print(f\"\\nConverting {category}...\")\n",
        "                category_start = time.time()\n",
        "\n",
        "                num_reviews = process_category(category, needed_columns, is_new_download=False)\n",
        "                if num_reviews > 0:\n",
        "                    completed.add(category)\n",
        "                    total_reviews += num_reviews\n",
        "                    save_progress(list(completed), total_reviews)\n",
        "\n",
        "                    elapsed = time.time() - category_start\n",
        "                    log_message = (\n",
        "                        f\"✓ Converted {category} in {elapsed:.1f} seconds\\n\"\n",
        "                        f\"  Reviews: {num_reviews:,}\\n\"\n",
        "                        f\"  Running total: {total_reviews:,} reviews\"\n",
        "                    )\n",
        "                    print(log_message)\n",
        "\n",
        "                    with open(log_filename, 'a') as f:\n",
        "                        f.write(f\"{datetime.now()}: {log_message}\\n\")\n",
        "\n",
        "                clear_memory()\n",
        "\n",
        "        # Continue with remaining categories\n",
        "        if remaining:\n",
        "            print_status(f\"\\nProcessing {len(remaining)} remaining categories...\", is_header=True)\n",
        "\n",
        "            for idx, category in enumerate(remaining, 1):\n",
        "                if category in completed:\n",
        "                    print(f\"Skipping {category} - already completed\")\n",
        "                    continue\n",
        "\n",
        "                print(f\"\\n[{idx}/{len(remaining)}] Processing {category}...\")\n",
        "                category_start = time.time()\n",
        "\n",
        "                num_reviews = process_category(category, needed_columns, is_new_download=True)\n",
        "                if num_reviews > 0:\n",
        "                    completed.add(category)\n",
        "                    total_reviews += num_reviews\n",
        "                    save_progress(list(completed), total_reviews)\n",
        "\n",
        "                    elapsed = time.time() - category_start\n",
        "                    log_message = (\n",
        "                        f\"✓ Completed {category} in {elapsed:.1f} seconds\\n\"\n",
        "                        f\"  Reviews: {num_reviews:,}\\n\"\n",
        "                        f\"  Running total: {total_reviews:,} reviews\"\n",
        "                    )\n",
        "                    print(log_message)\n",
        "\n",
        "                    with open(log_filename, 'a') as f:\n",
        "                        f.write(f\"{datetime.now()}: {log_message}\\n\")\n",
        "\n",
        "                if idx % 5 == 0:\n",
        "                    print_status(\"\\nPROGRESS UPDATE\", is_header=True)\n",
        "                    elapsed_total = (time.time() - start_time) / 60\n",
        "                    print(f\"Completed {idx}/{len(remaining)} new categories\")\n",
        "                    print(f\"Total time: {elapsed_total:.1f} minutes\")\n",
        "                    print(f\"Total reviews so far: {total_reviews:,}\")\n",
        "                    print(f\"Free disk space: {check_disk_space()} GB\")\n",
        "                    check_storage_usage()\n",
        "\n",
        "                clear_memory()\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nProcess interrupted by user\")\n",
        "        save_progress(list(completed), total_reviews)\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(f\"\\nUnexpected error: {str(e)}\")\n",
        "        save_progress(list(completed), total_reviews)\n",
        "        raise\n",
        "\n",
        "    total_time = (time.time() - start_time) / 60\n",
        "    print_status(\"\\nPROCESS COMPLETE\", is_header=True)\n",
        "    print(f\"Total time: {total_time:.1f} minutes\")\n",
        "    print(f\"Total reviews processed: {total_reviews:,}\")\n",
        "    print(f\"Log file: {log_filename}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print_status(\"AMAZON REVIEWS PROCESSOR\", is_header=True)\n",
        "\n",
        "    # Show initial storage status\n",
        "    check_storage_usage()\n",
        "\n",
        "    # Process categories\n",
        "    try:\n",
        "        transition_and_continue()\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nProcess interrupted by user\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nProcess failed: {str(e)}\")\n",
        "    finally:\n",
        "        # Always verify files and show storage status at the end\n",
        "        verify_parquet_files()\n",
        "        check_storage_usage()\n",
        "        print(f\"\\nTotal space freed: {TOTAL_SPACE_FREED:.1f}GB\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}