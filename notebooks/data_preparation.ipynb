{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ld5A15dhMBIz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a6d35a1-54a5-4a47-8a9f-f814fdc48837"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "✓ Google Drive mounted successfully\n",
            "✓ Backup directory: /content/drive/MyDrive/amazon_reviews_backup\n",
            "✓ Sample directory: /content/drive/MyDrive/amazon_reviews_backup/sampled_data_3percent\n",
            "Found 34 parquet files to process\n",
            "==================================================\n",
            "\n",
            "\n",
            "Run data preparation pipeline? (yes/no): yes\n",
            "\n",
            "Step 1: Creating sample dataset...\n",
            "\n",
            "Found existing combined file: /content/drive/MyDrive/amazon_reviews_backup/sampled_data_3percent/amazon_reviews_sampled_combined.parquet\n",
            "Would you like to (K)eep it or create (N)ew sample? [K/N]: K\n",
            "\n",
            "Step 2: Combining sampled files...\n",
            "\n",
            "Found existing combined file: /content/drive/MyDrive/amazon_reviews_backup/sampled_data_3percent/amazon_reviews_sampled_combined.parquet\n",
            "Contains 16,681,866 reviews\n",
            "Would you like to (K)eep it or create (N)ew combined file? [K/N]: K\n",
            "\n",
            "Step 3: Creating train/val/test splits...\n",
            "\n",
            "Found existing splits:\n",
            "train_raw.parquet: 13,345,492 reviews\n",
            "val_raw.parquet: 1,668,187 reviews\n",
            "test_raw.parquet: 1,668,187 reviews\n",
            "\n",
            "Would you like to (K)eep existing splits or create (N)ew ones? [K/N]: K\n",
            "\n",
            "Verifying splits...\n",
            "✓ train: 13,345,492 reviews\n",
            "✓ val: 1,668,187 reviews\n",
            "✓ test: 1,668,187 reviews\n",
            "\n",
            "✓ All splits verified successfully\n",
            "Total reviews across splits: 16,681,866\n",
            "\n",
            "Step 5: Preprocessing splits...\n",
            "\n",
            "Found existing processed splits:\n",
            "train_processed.parquet: 11,896,753 reviews\n",
            "val_processed.parquet: 1,573,506 reviews\n",
            "test_processed.parquet: 1,508,019 reviews\n",
            "\n",
            "Would you like to (K)eep existing processed splits or create (N)ew ones? [K/N]: N\n",
            "\n",
            "Loading train split...\n",
            "Processing train split...\n",
            "Initial size: 13,345,492\n",
            "Preprocessing text...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11896753/11896753 [06:27<00:00, 30678.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final size: 11,896,753\n",
            "Final rating distribution:\n",
            "rating\n",
            "1.0    0.114896\n",
            "2.0    0.049886\n",
            "3.0    0.070137\n",
            "4.0    0.124744\n",
            "5.0    0.640337\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Sentiment distribution:\n",
            "sentiment\n",
            "Positive    0.765081\n",
            "Negative    0.164782\n",
            "Neutral     0.070137\n",
            "Name: proportion, dtype: float64\n",
            "Saved to: /content/drive/MyDrive/amazon_reviews_backup/sampled_data_3percent/train_processed.parquet\n",
            "\n",
            "Loading val split...\n",
            "Processing val split...\n",
            "Initial size: 1,668,187\n",
            "Preprocessing text...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1573506/1573506 [00:52<00:00, 29743.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final size: 1,573,506\n",
            "Final rating distribution:\n",
            "rating\n",
            "1.0    0.110005\n",
            "2.0    0.050611\n",
            "3.0    0.071405\n",
            "4.0    0.127973\n",
            "5.0    0.640005\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Sentiment distribution:\n",
            "sentiment\n",
            "Positive    0.767979\n",
            "Negative    0.160616\n",
            "Neutral     0.071405\n",
            "Name: proportion, dtype: float64\n",
            "Saved to: /content/drive/MyDrive/amazon_reviews_backup/sampled_data_3percent/val_processed.parquet\n",
            "\n",
            "Loading test split...\n",
            "Processing test split...\n",
            "Initial size: 1,668,187\n",
            "Preprocessing text...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1508019/1508019 [00:48<00:00, 30801.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final size: 1,508,019\n",
            "Final rating distribution:\n",
            "rating\n",
            "1.0    0.104980\n",
            "2.0    0.052796\n",
            "3.0    0.074464\n",
            "4.0    0.133478\n",
            "5.0    0.634282\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Sentiment distribution:\n",
            "sentiment\n",
            "Positive    0.767760\n",
            "Negative    0.157776\n",
            "Neutral     0.074464\n",
            "Name: proportion, dtype: float64\n",
            "Saved to: /content/drive/MyDrive/amazon_reviews_backup/sampled_data_3percent/test_processed.parquet\n",
            "\n",
            "Final Statistics:\n",
            "Total reviews before preprocessing: 16,681,866\n",
            "Total reviews after preprocessing: 14,978,278\n",
            "Total reviews removed: 1,703,588 (10.2%)\n",
            "\n",
            "🎉 Data preparation completed successfully!\n",
            "\n",
            "Cleaning up memory...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import gc\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Mount Google Drive (safely)\n",
        "try:\n",
        "    # Check if already mounted\n",
        "    if not os.path.exists('/content/drive'):\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "    DRIVE_PATH = \"/content/drive/MyDrive/amazon_reviews_backup\"\n",
        "    SAMPLE_PATH = f\"{DRIVE_PATH}/sampled_data_3percent\"\n",
        "\n",
        "    # Create directories if they don't exist\n",
        "    os.makedirs(DRIVE_PATH, exist_ok=True)\n",
        "    os.makedirs(SAMPLE_PATH, exist_ok=True)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"✓ Google Drive mounted successfully\")\n",
        "    print(\"✓ Backup directory:\", DRIVE_PATH)\n",
        "    print(\"✓ Sample directory:\", SAMPLE_PATH)\n",
        "\n",
        "    # Verify files are accessible\n",
        "    parquet_files = [f for f in os.listdir(DRIVE_PATH) if f.endswith('.parquet')]\n",
        "    print(f\"Found {len(parquet_files)} parquet files to process\")\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"✗ Error mounting Google Drive:\", str(e))\n",
        "    raise\n",
        "\n",
        "# Progress tracking functions\n",
        "def save_sampling_progress(processed_files):\n",
        "    \"\"\"Save sampling progress to a JSON file\"\"\"\n",
        "    progress_file = f\"{SAMPLE_PATH}/sampling_progress.json\"\n",
        "    with open(progress_file, 'w') as f:\n",
        "        json.dump({\n",
        "            'processed_files': processed_files,\n",
        "            'total_processed': len(processed_files)\n",
        "        }, f)\n",
        "\n",
        "def load_sampling_progress():\n",
        "    \"\"\"Load previous sampling progress if it exists\"\"\"\n",
        "    progress_file = f\"{SAMPLE_PATH}/sampling_progress.json\"\n",
        "    if os.path.exists(progress_file):\n",
        "        with open(progress_file, 'r') as f:\n",
        "            return json.load(f)\n",
        "    return {'processed_files': [], 'total_processed': 0}\n",
        "\n",
        "def save_combination_progress(processed_files, total_rows):\n",
        "    \"\"\"Save combination progress\"\"\"\n",
        "    progress_file = f\"{SAMPLE_PATH}/combination_progress.json\"\n",
        "    with open(progress_file, 'w') as f:\n",
        "        json.dump({\n",
        "            'processed_files': processed_files,\n",
        "            'total_rows': total_rows\n",
        "        }, f)\n",
        "\n",
        "def load_combination_progress():\n",
        "    \"\"\"Load combination progress if exists\"\"\"\n",
        "    progress_file = f\"{SAMPLE_PATH}/combination_progress.json\"\n",
        "    if os.path.exists(progress_file):\n",
        "        with open(progress_file, 'r') as f:\n",
        "            return json.load(f)\n",
        "    return {'processed_files': [], 'total_rows': 0}\n",
        "\n",
        "def check_existing_splits():\n",
        "    \"\"\"Check if splits already exist\"\"\"\n",
        "    split_files = ['train_raw.parquet', 'val_raw.parquet', 'test_raw.parquet']\n",
        "    existing_splits = [f for f in split_files if os.path.exists(f\"{SAMPLE_PATH}/{f}\")]\n",
        "    if existing_splits:\n",
        "        print(\"\\nFound existing splits:\")\n",
        "        for split in existing_splits:\n",
        "            df = pd.read_parquet(f\"{SAMPLE_PATH}/{split}\")\n",
        "            print(f\"{split}: {len(df):,} reviews\")\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def create_sample_dataset(sample_fraction=0.03):\n",
        "    \"\"\"Create a 3% sample dataset with progress tracking\"\"\"\n",
        "\n",
        "    # Check if combined file already exists\n",
        "    combined_file = f\"{SAMPLE_PATH}/amazon_reviews_sampled_combined.parquet\"\n",
        "    if os.path.exists(combined_file):\n",
        "        print(f\"\\nFound existing combined file: {combined_file}\")\n",
        "        response = input(\"Would you like to (K)eep it or create (N)ew sample? [K/N]: \").lower()\n",
        "        if response == 'k':\n",
        "            return pd.read_parquet(combined_file)\n",
        "\n",
        "    # Create sample directory if it doesn't exist\n",
        "    os.makedirs(SAMPLE_PATH, exist_ok=True)\n",
        "\n",
        "    # Get all source files\n",
        "    files = sorted([f for f in os.listdir(DRIVE_PATH)\n",
        "                   if f.endswith('.parquet') and 'sampled' not in f])\n",
        "\n",
        "    # Load previous progress\n",
        "    progress = load_sampling_progress()\n",
        "    processed_files = progress['processed_files']\n",
        "\n",
        "    if processed_files:\n",
        "        print(f\"\\nFound previous progress: {len(processed_files)}/{len(files)} files processed\")\n",
        "        response = input(\"Would you like to (R)esume, (S)tart over, or (C)ancel? [R/S/C]: \").lower()\n",
        "        if response == 'c':\n",
        "            return None\n",
        "        elif response == 's':\n",
        "            processed_files = []\n",
        "            # Clear the sampled directory except progress file\n",
        "            for f in os.listdir(SAMPLE_PATH):\n",
        "                if f != 'sampling_progress.json':\n",
        "                    os.remove(f\"{SAMPLE_PATH}/{f}\")\n",
        "\n",
        "    print(f\"\\nFound {len(files)} category files\")\n",
        "    print(f\"Will process {len(files) - len(processed_files)} remaining files\")\n",
        "    total_sampled = 0\n",
        "\n",
        "    try:\n",
        "        for file in tqdm(files, desc=\"Sampling categories\"):\n",
        "            if file in processed_files:\n",
        "                print(f\"\\nSkipping {file} - already processed\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                print(f\"\\nProcessing: {file}\")\n",
        "\n",
        "                # Read and sample the file\n",
        "                df = pd.read_parquet(f'{DRIVE_PATH}/{file}')\n",
        "                original_size = len(df)\n",
        "                sample_size = int(original_size * sample_fraction)\n",
        "\n",
        "                sampled_df = df.sample(n=sample_size)\n",
        "\n",
        "                # Save sampled data\n",
        "                output_file = f'{SAMPLE_PATH}/{file}'\n",
        "                sampled_df.to_parquet(output_file)\n",
        "\n",
        "                # Print stats\n",
        "                file_size_mb = os.path.getsize(output_file) / (1024 * 1024)\n",
        "                print(f\"Original reviews: {original_size:,}\")\n",
        "                print(f\"Sampled reviews: {sample_size:,}\")\n",
        "                print(f\"File size: {file_size_mb:.2f}MB\")\n",
        "\n",
        "                total_sampled += sample_size\n",
        "\n",
        "                # Update progress\n",
        "                processed_files.append(file)\n",
        "                save_sampling_progress(processed_files)\n",
        "\n",
        "                del df, sampled_df\n",
        "                gc.collect()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error with {file}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        return total_sampled\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError during processing: {str(e)}\")\n",
        "        print(\"Progress has been saved - you can resume later\")\n",
        "        return None\n",
        "\n",
        "def combine_sampled_files():\n",
        "    \"\"\"Combine all sampled files with progress tracking\"\"\"\n",
        "\n",
        "    # Check if combined file already exists\n",
        "    combined_file = f\"{SAMPLE_PATH}/amazon_reviews_sampled_combined.parquet\"\n",
        "    if os.path.exists(combined_file):\n",
        "        print(f\"\\nFound existing combined file: {combined_file}\")\n",
        "        df = pd.read_parquet(combined_file)\n",
        "        print(f\"Contains {len(df):,} reviews\")\n",
        "        response = input(\"Would you like to (K)eep it or create (N)ew combined file? [K/N]: \").lower()\n",
        "        if response == 'k':\n",
        "            return df\n",
        "\n",
        "    # Get all sampled files\n",
        "    files = sorted([f for f in os.listdir(SAMPLE_PATH)\n",
        "                   if f.endswith('.parquet') and 'combined' not in f])\n",
        "\n",
        "    print(f\"\\nFound {len(files)} sampled files to combine\")\n",
        "\n",
        "    # Check for existing progress\n",
        "    progress = load_combination_progress()\n",
        "    processed_files = progress['processed_files']\n",
        "    total_rows = progress['total_rows']\n",
        "\n",
        "    if processed_files:\n",
        "        print(f\"\\nFound previous progress: {len(processed_files)}/{len(files)} files combined\")\n",
        "        print(f\"Current combined rows: {total_rows:,}\")\n",
        "        response = input(\"Would you like to (R)esume, (S)tart over, or (C)ancel? [R/S/C]: \").lower()\n",
        "        if response == 'c':\n",
        "            return None\n",
        "        elif response == 's':\n",
        "            processed_files = []\n",
        "            total_rows = 0\n",
        "            if os.path.exists(combined_file):\n",
        "                os.remove(combined_file)\n",
        "\n",
        "    try:\n",
        "        # If resuming, load existing combined file\n",
        "        if processed_files and os.path.exists(combined_file):\n",
        "            print(\"\\nLoading existing combined file...\")\n",
        "            combined_df = pd.read_parquet(combined_file)\n",
        "        else:\n",
        "            combined_df = None\n",
        "\n",
        "        # Process remaining files\n",
        "        remaining_files = [f for f in files if f not in processed_files]\n",
        "        print(f\"\\nProcessing {len(remaining_files)} remaining files...\")\n",
        "\n",
        "        for file in tqdm(remaining_files, desc=\"Combining files\"):\n",
        "            try:\n",
        "                print(f\"\\nProcessing: {file}\")\n",
        "\n",
        "                # Read file\n",
        "                df = pd.read_parquet(f\"{SAMPLE_PATH}/{file}\")\n",
        "                print(f\"File size: {len(df):,} reviews\")\n",
        "\n",
        "                # Combine\n",
        "                if combined_df is None:\n",
        "                    combined_df = df\n",
        "                else:\n",
        "                    combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
        "\n",
        "                total_rows = len(combined_df)\n",
        "                print(f\"Combined size: {total_rows:,} reviews\")\n",
        "\n",
        "                # Save progress\n",
        "                processed_files.append(file)\n",
        "                save_combination_progress(processed_files, total_rows)\n",
        "\n",
        "                # Save combined file after each successful addition\n",
        "                combined_df.to_parquet(combined_file)\n",
        "                print(f\"Saved combined file: {total_rows:,} reviews\")\n",
        "\n",
        "                del df\n",
        "                gc.collect()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {file}: {str(e)}\")\n",
        "                print(\"Progress saved - you can resume from here\")\n",
        "                return None\n",
        "\n",
        "        return combined_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError during combination: {str(e)}\")\n",
        "        print(\"Progress saved - you can resume later\")\n",
        "        return None\n",
        "\n",
        "def create_data_splits(test_size=0.1, val_size=0.1, random_state=42):\n",
        "    \"\"\"Create train/val/test splits with checks for existing splits\"\"\"\n",
        "\n",
        "    # Check for existing splits\n",
        "    if check_existing_splits():\n",
        "        response = input(\"\\nWould you like to (K)eep existing splits or create (N)ew ones? [K/N]: \").lower()\n",
        "        if response == 'k':\n",
        "            return (\n",
        "                pd.read_parquet(f\"{SAMPLE_PATH}/train_raw.parquet\"),\n",
        "                pd.read_parquet(f\"{SAMPLE_PATH}/val_raw.parquet\"),\n",
        "                pd.read_parquet(f\"{SAMPLE_PATH}/test_raw.parquet\")\n",
        "            )\n",
        "\n",
        "    print(\"\\nCreating new splits...\")\n",
        "\n",
        "    # Load combined dataset\n",
        "    combined_file = f\"{SAMPLE_PATH}/amazon_reviews_sampled_combined.parquet\"\n",
        "    if not os.path.exists(combined_file):\n",
        "        print(\"Error: Combined dataset not found. Please run combine_sampled_files() first.\")\n",
        "        return None\n",
        "\n",
        "    df = pd.read_parquet(combined_file)\n",
        "    print(f\"Initial dataset size: {len(df):,}\")\n",
        "\n",
        "    # First split\n",
        "    train_val, test = train_test_split(\n",
        "        df,\n",
        "        test_size=test_size,\n",
        "        random_state=random_state,\n",
        "        stratify=df['rating']\n",
        "    )\n",
        "    print(f\"\\nAfter first split:\")\n",
        "    print(f\"Train_val size: {len(train_val):,}\")\n",
        "    print(f\"Test size: {len(test):,}\")\n",
        "    print(f\"Total: {len(train_val) + len(test):,}\")\n",
        "\n",
        "    # Second split\n",
        "    val_size_adjusted = val_size / (1 - test_size)\n",
        "    train, val = train_test_split(\n",
        "        train_val,\n",
        "        test_size=val_size_adjusted,\n",
        "        random_state=random_state,\n",
        "        stratify=train_val['rating']\n",
        "    )\n",
        "    print(f\"\\nAfter second split:\")\n",
        "    print(f\"Train size: {len(train):,}\")\n",
        "    print(f\"Val size: {len(val):,}\")\n",
        "    print(f\"Test size: {len(test):,}\")\n",
        "    print(f\"Total: {len(train) + len(val) + len(test):,}\")\n",
        "\n",
        "    # Save splits with size verification\n",
        "    print(\"\\nSaving splits...\")\n",
        "    for name, data in [('train', train), ('val', val), ('test', test)]:\n",
        "        output_file = f\"{SAMPLE_PATH}/{name}_raw.parquet\"\n",
        "        before_save = len(data)\n",
        "        data.to_parquet(output_file)\n",
        "        after_save = len(pd.read_parquet(output_file))\n",
        "        print(f\"{name}: {before_save:,} → {after_save:,} reviews\")\n",
        "\n",
        "    return train, val, test\n",
        "\n",
        "def verify_splits():\n",
        "    \"\"\"Verify the integrity of raw splits\"\"\"\n",
        "    print(\"\\nVerifying splits...\")\n",
        "    splits = {\n",
        "        'train': 'train_raw.parquet',\n",
        "        'val': 'val_raw.parquet',\n",
        "        'test': 'test_raw.parquet'\n",
        "    }\n",
        "\n",
        "    all_valid = True\n",
        "    total_reviews = 0\n",
        "\n",
        "    for split_name, filename in splits.items():\n",
        "        path = f\"{SAMPLE_PATH}/{filename}\"\n",
        "        if os.path.exists(path):\n",
        "            try:\n",
        "                df = pd.read_parquet(path)\n",
        "                num_reviews = len(df)\n",
        "                total_reviews += num_reviews\n",
        "                print(f\"✓ {split_name}: {num_reviews:,} reviews\")\n",
        "\n",
        "                # Only verify presence of required columns for raw data\n",
        "                expected_cols = {'text', 'rating', 'category'}\n",
        "                missing_cols = expected_cols - set(df.columns)\n",
        "                if missing_cols:\n",
        "                    print(f\"✗ {split_name}: Missing columns: {missing_cols}\")\n",
        "                    all_valid = False\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"✗ Error reading {split_name}: {str(e)}\")\n",
        "                all_valid = False\n",
        "        else:\n",
        "            print(f\"✗ Missing {split_name} split\")\n",
        "            all_valid = False\n",
        "\n",
        "    if all_valid:\n",
        "        print(f\"\\n✓ All splits verified successfully\")\n",
        "        print(f\"Total reviews across splits: {total_reviews:,}\")\n",
        "\n",
        "    return all_valid\n",
        "\n",
        "def preprocess_dataset():\n",
        "    \"\"\"Preprocess the existing train/val/test splits\"\"\"\n",
        "\n",
        "    def check_existing_processed_splits():\n",
        "        \"\"\"Check if processed splits already exist\"\"\"\n",
        "        files = ['train_processed.parquet', 'val_processed.parquet', 'test_processed.parquet']\n",
        "        existing = all(os.path.exists(f\"{SAMPLE_PATH}/{f}\") for f in files)\n",
        "        if existing:\n",
        "            print(\"\\nFound existing processed splits:\")\n",
        "            for f in files:\n",
        "                size = len(pd.read_parquet(f\"{SAMPLE_PATH}/{f}\"))\n",
        "                print(f\"{f}: {size:,} reviews\")\n",
        "        return existing\n",
        "\n",
        "    if check_existing_processed_splits():\n",
        "        response = input(\"\\nWould you like to (K)eep existing processed splits or create (N)ew ones? [K/N]: \").lower()\n",
        "        if response == 'k':\n",
        "            return {\n",
        "                'train': pd.read_parquet(f\"{SAMPLE_PATH}/train_processed.parquet\"),\n",
        "                'val': pd.read_parquet(f\"{SAMPLE_PATH}/val_processed.parquet\"),\n",
        "                'test': pd.read_parquet(f\"{SAMPLE_PATH}/test_processed.parquet\")\n",
        "            }\n",
        "\n",
        "    def preprocess_text(text):\n",
        "        \"\"\"Preprocess review text\"\"\"\n",
        "        # Convert to string\n",
        "        text = str(text)\n",
        "\n",
        "        # Remove HTML\n",
        "        text = re.sub(r'<[^>]+>', ' ', text)\n",
        "\n",
        "        # Replace URLs and emails\n",
        "        text = re.sub(r'http[s]?://\\S+', '[URL]', text)\n",
        "        text = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', '[EMAIL]', text)\n",
        "\n",
        "        # Normalize whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = text.strip()\n",
        "\n",
        "        return text\n",
        "\n",
        "    # Process each split\n",
        "    splits = ['train', 'val', 'test']\n",
        "    processed_splits = {}\n",
        "    total_initial = 0\n",
        "    total_final = 0\n",
        "\n",
        "    for split_name in splits:\n",
        "        print(f\"\\nLoading {split_name} split...\")\n",
        "        raw_file = f\"{SAMPLE_PATH}/{split_name}_raw.parquet\"\n",
        "\n",
        "        if not os.path.exists(raw_file):\n",
        "            print(f\"Error: {raw_file} not found!\")\n",
        "            continue\n",
        "\n",
        "        df = pd.read_parquet(raw_file)\n",
        "        total_initial += len(df)\n",
        "\n",
        "        print(f\"Processing {split_name} split...\")\n",
        "        print(f\"Initial size: {len(df):,}\")\n",
        "\n",
        "        # Remove invalid data\n",
        "        if split_name == 'train':  # Only train has 0-ratings\n",
        "            df = df[df['rating'] > 0]\n",
        "        df = df[df['text'].notna()]  # Remove null text\n",
        "        df = df[df['text'].str.len() > 4]  # Remove very short text\n",
        "\n",
        "        # Remove duplicates\n",
        "        df = df.drop_duplicates(subset=['text', 'rating', 'category'], keep='first')\n",
        "\n",
        "        # Add unknown category token\n",
        "        df.loc[df['category'] == 'Unknown', 'category'] = '[UNKNOWN]'\n",
        "\n",
        "        # Preprocess text\n",
        "        print(\"Preprocessing text...\")\n",
        "        df['processed_text'] = df['text'].progress_apply(preprocess_text)\n",
        "\n",
        "        # Remove empty text after preprocessing\n",
        "        df = df[df['processed_text'].str.len() > 0]\n",
        "\n",
        "        # Add sentiment labels\n",
        "        df['sentiment'] = df['rating'].map({\n",
        "            1: 'Negative',\n",
        "            2: 'Negative',\n",
        "            3: 'Neutral',\n",
        "            4: 'Positive',\n",
        "            5: 'Positive'\n",
        "        })\n",
        "\n",
        "        print(f\"Final size: {len(df):,}\")\n",
        "        print(\"Final rating distribution:\")\n",
        "        print(df['rating'].value_counts(normalize=True).sort_index())\n",
        "        print(\"\\nSentiment distribution:\")\n",
        "        print(df['sentiment'].value_counts(normalize=True))\n",
        "\n",
        "        # Save processed split\n",
        "        output_file = f\"{SAMPLE_PATH}/{split_name}_processed.parquet\"\n",
        "        df.to_parquet(output_file)\n",
        "        print(f\"Saved to: {output_file}\")\n",
        "\n",
        "        processed_splits[split_name] = df\n",
        "        total_final += len(df)\n",
        "\n",
        "        del df\n",
        "        gc.collect()\n",
        "\n",
        "    # Print final statistics\n",
        "    print(\"\\nFinal Statistics:\")\n",
        "    print(f\"Total reviews before preprocessing: {total_initial:,}\")\n",
        "    print(f\"Total reviews after preprocessing: {total_final:,}\")\n",
        "    print(f\"Total reviews removed: {total_initial - total_final:,} ({(total_initial - total_final)/total_initial*100:.1f}%)\")\n",
        "\n",
        "    return processed_splits\n",
        "\n",
        "# Enable tqdm for pandas\n",
        "tqdm.pandas()\n",
        "\n",
        "# Run the pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    if input(\"\\nRun data preparation pipeline? (yes/no): \").lower() == 'yes':\n",
        "        try:\n",
        "            # Step 1: Create sample dataset\n",
        "            print(\"\\nStep 1: Creating sample dataset...\")\n",
        "            total_sampled = create_sample_dataset()\n",
        "\n",
        "            if total_sampled is not None:\n",
        "                # Step 2: Combine sampled files\n",
        "                print(\"\\nStep 2: Combining sampled files...\")\n",
        "                combined_df = combine_sampled_files()\n",
        "\n",
        "                if combined_df is not None:\n",
        "                    # Step 3: Create splits\n",
        "                    print(\"\\nStep 3: Creating train/val/test splits...\")\n",
        "                    train, val, test = create_data_splits()\n",
        "\n",
        "                    # Step 4: Verify splits\n",
        "                    if not verify_splits():\n",
        "                        print(\"\\n⚠️ Warning: Some splits may be incomplete or corrupted\")\n",
        "                        print(\"Please check the issues above and resolve them before proceeding\")\n",
        "                    else:\n",
        "                        # Add Step 5: Preprocess splits\n",
        "                        print(\"\\nStep 5: Preprocessing splits...\")\n",
        "                        processed_splits = preprocess_dataset()\n",
        "                        print(\"\\n🎉 Data preparation completed successfully!\")\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n\\nProcess interrupted by user\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\n\\nError during processing: {str(e)}\")\n",
        "        finally:\n",
        "            print(\"\\nCleaning up memory...\")\n",
        "            gc.collect()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}